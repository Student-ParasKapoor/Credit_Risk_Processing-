# PySpark Credit Risk Analytics Pipeline

## üìå Overview
This project implements a **PySpark-based time-series credit risk analytics pipeline** that simulates real-world banking and financial data processing workflows.  
It focuses on **loan-level risk metrics**, **data quality validation**, and **time-series completeness checks** using distributed data processing.

The pipeline ingests raw loan data, computes portfolio-level risk metrics, validates data quality, and prepares outputs suitable for downstream analytics or reporting.

---

## üè¶ Business Problem
Banks manage large volumes of **loan-level periodic data** for:
- Portfolio exposure monitoring
- Credit risk assessment
- Regulatory and internal reporting

Data quality issues such as **duplicate records**, **null values**, or **missing reporting periods** can lead to incorrect risk calculations and financial exposure.  
This project demonstrates how such data can be **processed, validated, and analyzed at scale** using PySpark.

---

## üõ†Ô∏è Tech Stack
- **Python**
- **PySpark (Spark DataFrame API, Spark SQL)**
- **SQL-style aggregations**
- **Parquet / CSV**
- **VS Code**
- **Git**

---

## üìÇ Project Structure
```text
pyspark_credit_risk_pipeline/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ raw/
‚îÇ       ‚îî‚îÄ‚îÄ credit_risk_data.csv
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ metrics/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exposure/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ default_rate/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ delinquency_rate/
‚îÇ   ‚îî‚îÄ‚îÄ exceptions/
‚îÇ       ‚îú‚îÄ‚îÄ duplicates/
‚îÇ       ‚îú‚îÄ‚îÄ nulls/
‚îÇ       ‚îî‚îÄ‚îÄ missing_months/
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ spark_session.py
    ‚îú‚îÄ‚îÄ ingest.py
    ‚îú‚îÄ‚îÄ transform.py
    ‚îú‚îÄ‚îÄ validate.py
```
## üì• Data Ingestion

- Raw loan-level data is ingested from CSV using **PySpark**
- Schema is inferred automatically
- Each row represents **one loan in one reporting month**

**Key columns:**
- `loan_id`
- `customer_id`
- `report_month`
- `outstanding_balance`
- `default_flag`
- `days_past_due`
- `product_type`

---

## üìä Risk Metrics Implemented

customer_id

report_month

outstanding_balance

default_flag

days_past_due

product_type

### 1Ô∏è‚É£ Monthly Portfolio Exposure

Aggregates total outstanding balance per month

Represents overall portfolio exposure

### 2Ô∏è‚É£ Monthly Default Rate
Calculated as:
defaulted_loans / total_loans

- Based on loan-level default flag

### 3Ô∏è‚É£ Monthly Delinquency Rate (DPD > 30)
- Measures early-stage credit risk  
- Flags loans with `days_past_due > 30`

---

## ‚úÖ Data Quality & Validation

### üîπ Duplicate Detection
- Identifies duplicate records using:
loan_id + report_month

- Records are **flagged, not deleted**

### üîπ Null Checks
Flags records with null values in critical fields:
- `loan_id`
- `customer_id`
- `report_month`
- `outstanding_balance`
- `default_flag`

### üîπ Missing Month Validation
- Performs loan-level time-series completeness checks
- Generates expected months between first and last reporting period
- Detects gaps using left joins
- Prevents false positives by respecting loan lifecycle

---

## üíæ Output Persistence

- Clean metrics and exception datasets are written separately

**Output formats:**
- **Parquet** (recommended for Linux / production environments)
- **CSV** (can be used for local Windows execution)

> ‚ö†Ô∏è **Note:** On Windows systems, Spark may require additional Hadoop configuration (`winutils.exe`) for Parquet writes.  
> This limitation does not apply to Linux-based Spark clusters used in production.

---

## ‚ñ∂Ô∏è How to Run

### 1Ô∏è‚É£ Activate virtual environment
```bash
venv\Scripts\activate
```
### 2Ô∏è‚É£ Run transformations
```bash
python src/transform.py
```
### 3Ô∏è‚É£ Run validations
```bash
python src/validate.py
```
---

## üß† Key Concepts Demonstrated

- Distributed data processing with Spark
- Lazy evaluation
- Time-series aggregation
- Loan-level vs portfolio-level analytics
- Data quality validation in regulated environments
- Separation of clean outputs and exception datasets

---

## üöÄ Future Enhancements

- Customer-level aggregation
- Cloud storage integration (S3 / ADLS)
- Pipeline orchestration and scheduling
- Monitoring and alerting for data quality
- Unit tests for validation logic

---
## üë§ Author

**Paras Kapoor**  
Data / Risk Analytics  
Python | PySpark | SQL
